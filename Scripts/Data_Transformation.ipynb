{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_and_script(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Remove script and style tags completely\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "        \n",
    "\n",
    "    return soup.get_text(strip=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_handling(text):\n",
    "    # Dictionary of unicode escape sequences mapped to their actual characters\n",
    "    unicode_map = {\n",
    "        r'\\u2018': 'â€˜',  # Left single quote\n",
    "        r'\\u2019': 'â€™',  # Right single quote\n",
    "        r'\\u201c': 'â€œ',  # Left double quote\n",
    "        r'\\u201d': 'â€',  # Right double quote\n",
    "        r'\\u2013': 'â€“',  # En dash\n",
    "        r'\\u2014': 'â€”',  # Em dash\n",
    "        r'\\u2022': 'â€¢',  # Bullet\n",
    "        r'\\u2026': 'â€¦',  # Ellipsis\n",
    "        r'\\u00a0': ' ',  # Non-breaking space\n",
    "        r'\\u00b7': 'Â·',  # Middle dot\n",
    "        r'\\u00e9': 'Ã©',  # e acute\n",
    "        r'\\u00e2': 'Ã¢',  # a circumflex\n",
    "        r'\\u00e0': 'Ã ',  # a grave\n",
    "        r'\\u00e8': 'Ã¨',  # e grave\n",
    "        r'\\u00e7': 'Ã§',  # c cedilla\n",
    "        r'\\u00f4': 'Ã´',  # o circumflex\n",
    "        r'\\u00fb': 'Ã»',  # u circumflex\n",
    "        r'\\u00ee': 'Ã®',  # i circumflex\n",
    "        r'\\u00ef': 'Ã¯',  # i diaeresis\n",
    "        r'\\u00e4': 'Ã¤',  # a umlaut\n",
    "        r'\\u00f6': 'Ã¶',  # o umlaut\n",
    "        r'\\u00fc': 'Ã¼',  # u umlaut\n",
    "        r'\\u00df': 'ÃŸ',  # sharp s\n",
    "        r'\\u2082': 'â‚‚',  # subscript 2\n",
    "        r'\\u2083': 'â‚ƒ',  # subscript 3\n",
    "        r'\\u267b': '',         # Recycling symbol\n",
    "        r'\\ufe0f': '',         # Variation selector\n",
    "        # r'\\ud83d\\udd25': '',   # Fire emoji\n",
    "        # r'\\ud83c\\udf1f': '', \n",
    "        # r'\\u2744\\ufe0f': '',\n",
    "        r'\\u2744': '',\n",
    "        r'\\u2122': 'â„¢',\n",
    "        r'\\u27a1': '',\n",
    "        r'\\u20ac': 'â‚¬',\n",
    "        r'\\u201': '',\n",
    "        r'\\u2013': 'â€“',\n",
    "        r'\\u2014': 'â€”',\n",
    "        #r'\\ud83d\\udccd': '',\n",
    "        #r'\\ud83c\\udf89': '',\n",
    "        #r'\\ud83d\\udd17': '',\n",
    "        #r'\\ud83d\\udd0e': '',\n",
    "        #r'\\ud83d\\udcf8': '',\n",
    "        #r'\\ud83d\\udc49': '',\n",
    "        #r'\\ud83c\\udfa7': '',\n",
    "        #r'\\ud83e\\udd1d': '',\n",
    "        #r'\\u2714': '',\n",
    "        #r'\\ud83d\\udca1': '',\n",
    "        r'\\u23f0': '',\n",
    "        # r'\\ud83c\\udf88': '',\n",
    "        r'\\u2': '',\n",
    "        r'\\u201e': '',\n",
    "        r'\\u26a1': '',\n",
    "        # r'\\ud83d\\udd12': '',\n",
    "        # r'\\ud83d\\ude80': '',  # Unicode for \"ROCKET\" emoji (ðŸš€).\n",
    "        # r'\\ud83c\\u': '',  # Represents other emojis or special characters.\n",
    "        r'\\u25b6': '',  # Unicode for \"BLACK RIGHT-POINTING TRIANGLE\" (â–¶), used for video/play buttons.\n",
    "        r'\\u2b05': '',\n",
    "        r'\\u0130': '',\n",
    "        # r'\\ud83c\\udf2c': '',  # Unicode for \"TROPICAL STORM\" emoji (ðŸŒ€)\n",
    "        # r'\\ud83c\\uud83c': '',  # Represents other emojis or special characters.\n",
    "        \n",
    "    }\n",
    "\n",
    "    for code, char in unicode_map.items():\n",
    "        text = text.replace(code, char)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_matches(text):\n",
    "    # Regular expression to match Unicode escape sequences\n",
    "    unicode_pattern = r'\\\\u[0-9a-fA-F]{4}|\\\\U[0-9a-fA-F]{8}'\n",
    "\n",
    "    # Replace all matches with an empty string\n",
    "    updated_string = re.sub(unicode_pattern, '', text)\n",
    "\n",
    "    return updated_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def extract_date_ddmmyyyy(iso_datetime: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and formats the date portion from an ISO 8601 datetime string\n",
    "    into 'dd-mm-yyyy' format.\n",
    "\n",
    "    Parameters:\n",
    "        iso_datetime (str): An ISO 8601 datetime string (e.g., '2025-04-16T04:31:13Z').\n",
    "\n",
    "    Returns:\n",
    "        str: The date in 'dd-mm-yyyy' format.\n",
    "    \"\"\"\n",
    "    dt = datetime.fromisoformat(iso_datetime.replace(\"Z\", \"+00:00\"))\n",
    "    return dt.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_summary(summary):\n",
    "    max_length=200\n",
    "    # If summary is already short enough, return as is\n",
    "    if len(summary) <= max_length:\n",
    "        return summary\n",
    "    \n",
    "    # Cut the summary at max_length\n",
    "    truncated = summary[:max_length]\n",
    "    \n",
    "    # Find the last space to avoid splitting words\n",
    "    last_space = truncated.rfind(' ')\n",
    "    \n",
    "    # If no space found, just cut at max_length\n",
    "    if last_space == -1:\n",
    "        return truncated.rstrip() + \"...\"\n",
    "    \n",
    "    # Cut at the last space and append ellipsis\n",
    "    return truncated[:last_space].rstrip() + \"...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "api request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 500\n",
      "Request failed with status code 500\n",
      "\n",
      "âœ… Total articles fetched: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\203156\\Desktop\\Pipelines\\I2D_API\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'my.intelligence2day.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# API Query to test functionality (without pagination)\n",
    "\n",
    "# API base URL and static parameters\n",
    "api_url = \"https://my.intelligence2day.com/components/api/search.cfc\"\n",
    "params = {\n",
    "    \"method\": \"query\",\n",
    "    \"APIid\": \"I2DE_4880557FFC6ABA165C916880849F9CAC\",\n",
    "    \"authKey\": \"c51e7492-ab7f-46d8-9d10-edd4e434d2c1\",\n",
    "    \"customerGUID\": \"b6150206-d9b1-4963-8907-22b7695c0477\",\n",
    "    \"accessGroups\": \"8329\",\n",
    "    \"returnFields\": \"*\",\n",
    "    #\"queryString\": \"*:*\",      #Query for all records\n",
    "    \"queryString\": \"dateline:[NOW-2WEEKS TO NOW] AND topicId:135576\",  # Query for all records within time range\n",
    "    \"maxRows\": 10,  # Limit to x results\n",
    "    \"sort\": \"dateline desc\",  # Sort by \n",
    "\n",
    "}\n",
    "\n",
    "total_articles = 0\n",
    "all_articles = []  # To store all article data\n",
    "\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(api_url, params=params, verify=False)\n",
    "\n",
    "# Print the status code\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        data = response.json()  # Parse the response as JSON\n",
    "        print(\"Returned Data:\")\n",
    "\n",
    "        formatted_json = json.dumps(data, indent=4)\n",
    "        print(formatted_json)    # Print the raw JSON response\n",
    "            \n",
    "        articles = data.get(\"docs\", [])\n",
    "\n",
    "\n",
    "        if not articles:\n",
    "            print(\"No more articles returned.\")\n",
    "            \n",
    "\n",
    "        print(f\"Retrieved {len(articles)} articles\")\n",
    "\n",
    "        # Print the articles' title, summary, and URL\n",
    "        for i, article in enumerate(articles, 1):\n",
    "            title = article.get(\"headline\", \"No title\")\n",
    "            summary = article.get(\"summary\", \"No summary\")\n",
    "            url = article.get(\"attachmenturl\", \"No URL\")\n",
    "            date = article.get(\"dateline\", \"No date\")\n",
    "\n",
    "            all_articles.append({\"Title\": title, \"Summary\": summary, \"URL\": url, \"Date\": date})\n",
    "\n",
    "            print(f\"\\nArticle {total_articles + i}\")\n",
    "            print(f\"Title   : {title}\")\n",
    "            print(f\"Summary : {summary}\")\n",
    "            print(f\"URL     : {url}\")\n",
    "            print(f\"Date    : {date}\")\n",
    "\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Error: Response is not valid JSON.\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "    \n",
    "total_articles = len(all_articles)\n",
    "print(f\"\\nâœ… Total articles fetched: {total_articles}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estamos prontos para a Fispal 2025! De 27 a 30...</td>\n",
       "      <td>Estamos prontos para a Fispal 2025! De 27 a 30...</td>\n",
       "      <td>https://www.linkedin.com/feed/update/urn:li:ac...</td>\n",
       "      <td>2025-05-27T22:01:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In collaboration with North American Sustainab...</td>\n",
       "      <td>In collaboration with North American Sustainab...</td>\n",
       "      <td>https://www.linkedin.com/feed/update/urn:li:ac...</td>\n",
       "      <td>2025-05-27T19:01:46Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CO\\u2082 refrigeration doesn\\u2019t have to be...</td>\n",
       "      <td>CO\\u2082 refrigeration doesn\\u2019t have to be...</td>\n",
       "      <td>https://www.linkedin.com/feed/update/urn:li:ac...</td>\n",
       "      <td>2025-05-27T16:01:56Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Earlier in May, Copeland attended an inspiring...</td>\n",
       "      <td>Earlier in May, Copeland attended an inspiring...</td>\n",
       "      <td>https://www.linkedin.com/feed/update/urn:li:ac...</td>\n",
       "      <td>2025-05-27T14:06:18Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rail heat control in supermarket HVAC-R system...</td>\n",
       "      <td>Rail heat control in supermarket HVAC-R system...</td>\n",
       "      <td>https://www.linkedin.com/feed/update/urn:li:ac...</td>\n",
       "      <td>2025-05-27T11:20:03Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Estamos prontos para a Fispal 2025! De 27 a 30...   \n",
       "1  In collaboration with North American Sustainab...   \n",
       "2  CO\\u2082 refrigeration doesn\\u2019t have to be...   \n",
       "3  Earlier in May, Copeland attended an inspiring...   \n",
       "4  Rail heat control in supermarket HVAC-R system...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Estamos prontos para a Fispal 2025! De 27 a 30...   \n",
       "1  In collaboration with North American Sustainab...   \n",
       "2  CO\\u2082 refrigeration doesn\\u2019t have to be...   \n",
       "3  Earlier in May, Copeland attended an inspiring...   \n",
       "4  Rail heat control in supermarket HVAC-R system...   \n",
       "\n",
       "                                                 URL                  Date  \n",
       "0  https://www.linkedin.com/feed/update/urn:li:ac...  2025-05-27T22:01:57Z  \n",
       "1  https://www.linkedin.com/feed/update/urn:li:ac...  2025-05-27T19:01:46Z  \n",
       "2  https://www.linkedin.com/feed/update/urn:li:ac...  2025-05-27T16:01:56Z  \n",
       "3  https://www.linkedin.com/feed/update/urn:li:ac...  2025-05-27T14:06:18Z  \n",
       "4  https://www.linkedin.com/feed/update/urn:li:ac...  2025-05-27T11:20:03Z  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.DataFrame(all_articles)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Title'] = test_df['Title'].apply(remove_html_and_script)\n",
    "test_df['Title'] = test_df['Title'].apply(unicode_handling)\n",
    "test_df['Title'] = test_df['Title'].apply(remove_matches)\n",
    "test_df['Summary'] = test_df['Summary'].apply(unicode_handling)\n",
    "test_df['Summary'] = test_df['Summary'].apply(remove_matches)\n",
    "test_df['Summary'] = test_df['Summary'].apply(remove_html_and_script)\n",
    "test_df['Summary'] = test_df['Summary'].apply(truncate_summary)\n",
    "test_df['Date'] = test_df['Date'].apply(extract_date_ddmmyyyy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to excel and formatting the output so that it is in \"table\" format with name \"table1\"\n",
    "\n",
    "#current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Construct filename with current date\n",
    "#filename = f\"../updated_{current_date}.xlsx\"\n",
    "filename = f\"../updated.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(filename, engine=\"xlsxwriter\") as writer:\n",
    "    test_df.to_excel(writer, sheet_name=\"Sheet1\", index=False, startrow=0)\n",
    "\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"Sheet1\"]\n",
    "\n",
    "    # defining column settings\n",
    "    (max_row, max_col) = test_df.shape\n",
    "    column_settings = [{\"header\": col} for col in test_df.columns]\n",
    "\n",
    "    # Define table range & add table\n",
    "    worksheet.add_table(0, 0, max_row, max_col -1,{\n",
    "        \"columns\": column_settings,\n",
    "        \"name\": \"Table1\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def export_data(df):\n",
    "    file_path = \"../records.xlsx\"\n",
    "\n",
    "    # Check if file exists and determine mode and start row\n",
    "    if os.path.exists(file_path):\n",
    "        workbook = load_workbook(file_path)\n",
    "        sheet = workbook.active\n",
    "        start_row = sheet.max_row\n",
    "        mode = \"a\"\n",
    "        sheet_exists_option = {\"if_sheet_exists\": \"overlay\"}\n",
    "        write_header = False\n",
    "    else:\n",
    "        start_row = 0\n",
    "        mode = \"w\"\n",
    "        sheet_exists_option = {}\n",
    "        write_header = True\n",
    "\n",
    "    # Use unpacking to only include 'if_sheet_exists' when needed\n",
    "    with pd.ExcelWriter(file_path, engine=\"openpyxl\", mode=mode, **sheet_exists_option) as writer:\n",
    "        df.to_excel(writer, sheet_name=\"Sheet1\", index=False, startrow=start_row,header=write_header)\n",
    "\n",
    "    print(f\"Data written to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ../records.xlsx\n"
     ]
    }
   ],
   "source": [
    "export_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ../records.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "records(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def records(df):\n",
    "    file_path = \"../records.xlsx\"\n",
    "\n",
    "\n",
    "    if os.path.exists(file_path):            \n",
    "        excel_records = pd.read_excel(file_path, sheet_name=\"Sheet1\", parse_dates=[\"Date\"])\n",
    "        latest_date = excel_records[\"Date\"].max()\n",
    "\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        new_data = df[df[\"Date\"] > pd.to_datetime(latest_date)]\n",
    "        new_data = new_data.reset_index(drop=True)\n",
    "\n",
    "        # Format 'Date' column as yyyy-mm-dd string\n",
    "        new_data['Date'] = new_data['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        print(f\"New data since {latest_date}:\")\n",
    "        print(new_data.head())\n",
    "\n",
    "        if not new_data.empty:\n",
    "            workbook = load_workbook(file_path)\n",
    "            sheet = workbook.active\n",
    "            start_row = sheet.max_row\n",
    "            mode = \"a\"\n",
    "            sheet_exists_option = {\"if_sheet_exists\": \"overlay\"}\n",
    "            write_header = False\n",
    "                # Use unpacking to only include 'if_sheet_exists' when needed\n",
    "            with pd.ExcelWriter(file_path, engine=\"openpyxl\", mode=mode, **sheet_exists_option) as writer:\n",
    "                new_data.to_excel(writer, sheet_name=\"Sheet1\", index=False, startrow=start_row,header=write_header)\n",
    "            print(f\"Data appended to {file_path}\")\n",
    "        else:\n",
    "            print(\"there is no new data to append\")\n",
    "    else:\n",
    "        # Format 'Date column as yyyy-mm-dd string\n",
    "        df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        start_row = 0\n",
    "        mode = \"w\"\n",
    "        sheet_exists_option = {}\n",
    "        write_header = True\n",
    "        # Use unpacking to only include 'if_sheet_exists' when needed\n",
    "        with pd.ExcelWriter(file_path, engine=\"openpyxl\", mode=mode, **sheet_exists_option) as writer:\n",
    "            df.to_excel(writer, sheet_name=\"Sheet1\", index=False, startrow=start_row,header=write_header)\n",
    "        print(f\"Data written to {file_path}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    10 non-null     object\n",
      " 1   Summary  10 non-null     object\n",
      " 2   URL      10 non-null     object\n",
      " 3   Date     10 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 452.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "def email_records(df):\n",
    "    file_path = \"email_records.xlsx\"\n",
    "\n",
    "    #convert 'Date' column to datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    #creating new dataframe based upon filter of last 7 days worth of articles \n",
    "    one_week_ago = datetime.now() - pd.Timedelta(days=7)\n",
    "    recent_data = df[df['Date'] >= one_week_ago]\n",
    "    recent_data = recent_data.reset_index(drop=True)\n",
    "    \n",
    "    with pd.ExcelWriter(file_path, engine=\"xlsxwriter\") as writer:\n",
    "        recent_data.to_excel(writer, sheet_name=\"Sheet1\", index=False, startrow=0)\n",
    "\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[\"Sheet1\"]\n",
    "\n",
    "        # defining column settings\n",
    "        (max_row, max_col) = df.shape\n",
    "        column_settings = [{\"header\": col} for col in df.columns]\n",
    "\n",
    "        # Define table range & add table\n",
    "        worksheet.add_table(0, 0, max_row, max_col -1,{\n",
    "            \"columns\": column_settings,\n",
    "            \"name\": \"Table1\",\n",
    "    })\n",
    "\n",
    "    print(f\"Data written to {file_path}\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to email_records.xlsx\n"
     ]
    }
   ],
   "source": [
    "email_records(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
