{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from api_call import api_call\n",
    "from data_transformation import data_transformations\n",
    "from data_export import export_data\n",
    "\n",
    "\n",
    "print(\"running api call function\")\n",
    "all_articles = api_call()\n",
    "\n",
    "print(\"running data transformation function\")\n",
    "transformed_data = data_transformations(all_articles)\n",
    "\n",
    "print(\"running data export function\")\n",
    "export_data(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api_call.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api_call.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# API Query with pagination control (ability to iterate through all pages/records)\n",
    "\n",
    "# API base URL and static parameters\n",
    "def api_call():\n",
    "    api_url = \"https://my.intelligence2day.com/components/api/search.cfc\"\n",
    "    params = {\n",
    "        \"method\": \"query\",\n",
    "        \"APIid\": \"I2DE_4880557FFC6ABA165C916880849F9CAC\",\n",
    "        \"authKey\": \"c51e7492-ab7f-46d8-9d10-edd4e434d2c1\",\n",
    "        \"customerGUID\": \"b6150206-d9b1-4963-8907-22b7695c0477\",\n",
    "        \"accessGroups\": \"8329\",\n",
    "        \"returnFields\": \"*\",\n",
    "        # \"queryString\": \"*:*\",  # Query for all records\n",
    "        \"queryString\": \"dateline:[NOW-2MONTHS TO NOW] AND topicId:135576\",  # Query for all records within time range\n",
    "        \"maxRows\": 500,  # Limit to x results\n",
    "        \"sort\": \"dateline desc\",  # Sort by UID in descending order\n",
    "    }\n",
    "\n",
    "    # Pagination control\n",
    "    cursor = \"*\"  # Start with an empty cursor for the first request\n",
    "    has_more = True\n",
    "    total_articles = 0\n",
    "    page = 1\n",
    "    all_articles = []  # To store all article data\n",
    "\n",
    "    while has_more:\n",
    "        time.sleep(2)\n",
    "        print(f\"\\n--- Fetching Page {page} ---\")\n",
    "\n",
    "        # Update the cursor in the request parameters for pagination\n",
    "        params[\"cursorMark\"] = cursor\n",
    "\n",
    "        # Make the request\n",
    "        response = requests.get(api_url, params=params, verify=False)\n",
    "\n",
    "        # Print the status code\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()  # Parse the response as JSON\n",
    "                print(\"Returned Data:\")\n",
    "                \n",
    "                formatted_json = json.dumps(data, indent=4)\n",
    "                print(formatted_json)    # Print the raw JSON response\n",
    "                \n",
    "                articles = data.get(\"docs\", [])\n",
    "                next_cursor = data.get(\"nextCursormark\", None)\n",
    "\n",
    "                if not articles:\n",
    "                    print(\"No more articles returned.\")\n",
    "                    break\n",
    "\n",
    "                print(f\"Retrieved {len(articles)} articles on page {page}.\")\n",
    "\n",
    "                # Print the articles' title, summary, and URL\n",
    "                for i, article in enumerate(articles, 1):\n",
    "                    title = article.get(\"headline\", \"No title\")\n",
    "                    summary = article.get(\"summary\", \"No summary\")\n",
    "                    url = article.get(\"attachmenturl\", \"No URL\")\n",
    "                    date = article.get(\"dateline\", \"No date\")\n",
    "\n",
    "                    all_articles.append({\"Title\": title, \"Summary\": summary, \"URL\": url, \"Date\": date})\n",
    "\n",
    "                    # print(f\"\\nArticle {total_articles + i}\")\n",
    "                    # print(f\"Title   : {title}\")\n",
    "                    # print(f\"Summary : {summary}\")\n",
    "                    # print(f\"URL     : {url}\")\n",
    "                    # print(f\"Date    : {date}\")\n",
    "\n",
    "                total_articles += len(articles)\n",
    "                page += 1\n",
    "\n",
    "                # Prepare for the next iteration with the nextCursormark\n",
    "                if next_cursor:\n",
    "                    cursor = next_cursor  # Update the cursor for the next request\n",
    "                else:\n",
    "                    has_more = False  # No more pages, end the loop\n",
    "\n",
    "            except ValueError:\n",
    "                print(\"Error: Response is not valid JSON.\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Request failed with status code {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n‚úÖ Total articles fetched: {total_articles}\")\n",
    "    return all_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_transformation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_transformation.py\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_and_script(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Remove script and style tags completely\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "        \n",
    "\n",
    "    return soup.get_text(strip=False)\n",
    "\n",
    "def unicode_handling(text):\n",
    "    # Dictionary of unicode escape sequences mapped to their actual characters\n",
    "    unicode_map = {\n",
    "        r'\\u2018': '‚Äò',  # Left single quote\n",
    "        r'\\u2019': '‚Äô',  # Right single quote\n",
    "        r'\\u201c': '‚Äú',  # Left double quote\n",
    "        r'\\u201d': '‚Äù',  # Right double quote\n",
    "        r'\\u2013': '‚Äì',  # En dash\n",
    "        r'\\u2014': '‚Äî',  # Em dash\n",
    "        r'\\u2022': '‚Ä¢',  # Bullet\n",
    "        r'\\u2026': '‚Ä¶',  # Ellipsis\n",
    "        r'\\u00a0': ' ',  # Non-breaking space\n",
    "        r'\\u00b7': '¬∑',  # Middle dot\n",
    "        r'\\u00e9': '√©',  # e acute\n",
    "        r'\\u00e2': '√¢',  # a circumflex\n",
    "        r'\\u00e0': '√†',  # a grave\n",
    "        r'\\u00e8': '√®',  # e grave\n",
    "        r'\\u00e7': '√ß',  # c cedilla\n",
    "        r'\\u00f4': '√¥',  # o circumflex\n",
    "        r'\\u00fb': '√ª',  # u circumflex\n",
    "        r'\\u00ee': '√Æ',  # i circumflex\n",
    "        r'\\u00ef': '√Ø',  # i diaeresis\n",
    "        r'\\u00e4': '√§',  # a umlaut\n",
    "        r'\\u00f6': '√∂',  # o umlaut\n",
    "        r'\\u00fc': '√º',  # u umlaut\n",
    "        r'\\u00df': '√ü',  # sharp s\n",
    "        r'\\u2082': '‚ÇÇ',  # subscript 2\n",
    "        r'\\u2083': '‚ÇÉ',  # subscript 3\n",
    "        r'\\u267b': '',         # Recycling symbol\n",
    "        r'\\ufe0f': '',         # Variation selector\n",
    "        # r'\\ud83d\\udd25': '',   # Fire emoji\n",
    "        # r'\\ud83c\\udf1f': '', \n",
    "        # r'\\u2744\\ufe0f': '',\n",
    "        r'\\u2744': '',\n",
    "        r'\\u2122': '‚Ñ¢',\n",
    "        r'\\u27a1': '',\n",
    "        r'\\u20ac': '‚Ç¨',\n",
    "        r'\\u201': '',\n",
    "        r'\\u2013': '‚Äì',\n",
    "        r'\\u2014': '‚Äî',\n",
    "        #r'\\ud83d\\udccd': '',\n",
    "        #r'\\ud83c\\udf89': '',\n",
    "        #r'\\ud83d\\udd17': '',\n",
    "        #r'\\ud83d\\udd0e': '',\n",
    "        #r'\\ud83d\\udcf8': '',\n",
    "        #r'\\ud83d\\udc49': '',\n",
    "        #r'\\ud83c\\udfa7': '',\n",
    "        #r'\\ud83e\\udd1d': '',\n",
    "        #r'\\u2714': '',\n",
    "        #r'\\ud83d\\udca1': '',\n",
    "        r'\\u23f0': '',\n",
    "        # r'\\ud83c\\udf88': '',\n",
    "        r'\\u2': '',\n",
    "        r'\\u201e': '',\n",
    "        r'\\u26a1': '',\n",
    "        # r'\\ud83d\\udd12': '',\n",
    "        # r'\\ud83d\\ude80': '',  # Unicode for \"ROCKET\" emoji (üöÄ).\n",
    "        # r'\\ud83c\\u': '',  # Represents other emojis or special characters.\n",
    "        r'\\u25b6': '',  # Unicode for \"BLACK RIGHT-POINTING TRIANGLE\" (‚ñ∂), used for video/play buttons.\n",
    "        r'\\u2b05': '',\n",
    "        r'\\u0130': '',\n",
    "        # r'\\ud83c\\udf2c': '',  # Unicode for \"TROPICAL STORM\" emoji (üåÄ)\n",
    "        # r'\\ud83c\\uud83c': '',  # Represents other emojis or special characters.\n",
    "        \n",
    "    }\n",
    "\n",
    "    for code, char in unicode_map.items():\n",
    "        text = text.replace(code, char)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_matches(text):\n",
    "    # Regular expression to match Unicode escape sequences\n",
    "    unicode_pattern = r'\\\\u[0-9a-fA-F]{4}|\\\\U[0-9a-fA-F]{8}'\n",
    "\n",
    "    # Replace all matches with an empty string\n",
    "    updated_string = re.sub(unicode_pattern, '', text)\n",
    "\n",
    "    return updated_string\n",
    "\n",
    "\n",
    "def truncate_summary(summary):\n",
    "    max_length=200\n",
    "    # If summary is already short enough, return as is\n",
    "    if len(summary) <= max_length:\n",
    "        return summary\n",
    "    \n",
    "    # Cut the summary at max_length\n",
    "    truncated = summary[:max_length]\n",
    "    \n",
    "    # Find the last space to avoid splitting words\n",
    "    last_space = truncated.rfind(' ')\n",
    "    \n",
    "    # If no space found, just cut at max_length\n",
    "    if last_space == -1:\n",
    "        return truncated.rstrip() + \"...\"\n",
    "    \n",
    "    # Cut at the last space and append ellipsis\n",
    "    return truncated[:last_space].rstrip() + \"...\"\n",
    "\n",
    "\n",
    "\n",
    "def extract_date_ddmmyyyy(iso_datetime: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and formats the date portion from an ISO 8601 datetime string\n",
    "    into 'dd-mm-yyyy' format.\n",
    "\n",
    "    Parameters:\n",
    "        iso_datetime (str): An ISO 8601 datetime string (e.g., '2025-04-16T04:31:13Z').\n",
    "\n",
    "    Returns:\n",
    "        str: The date in 'dd-mm-yyyy' format.\n",
    "    \"\"\"\n",
    "    dt = datetime.fromisoformat(iso_datetime.replace(\"Z\", \"+00:00\"))\n",
    "    return dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "\n",
    "def data_transformations(all_articles):\n",
    "    print(\"Starting data transformations\")\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    df['Title'] = df['Title'].apply(remove_html_and_script)\n",
    "    df['Title'] = df['Title'].apply(unicode_handling)\n",
    "    df['Title'] = df['Title'].apply(remove_matches)\n",
    "    df['Summary'] = df['Summary'].apply(remove_html_and_script)\n",
    "    df['Summary'] = df['Summary'].apply(unicode_handling)\n",
    "    df['Summary'] = df['Summary'].apply(remove_matches)\n",
    "    df['Summary'] = df['Summary'].apply(truncate_summary)\n",
    "    df['Date'] = df['Date'].apply(extract_date_ddmmyyyy)\n",
    "\n",
    "    print(\"Data transformations completed\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_export.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_export.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def new_records(df):\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Construct filename with current date\n",
    "    filename = f\"updated.xlsx\"\n",
    "\n",
    "    with pd.ExcelWriter(filename, engine=\"xlsxwriter\") as writer:\n",
    "        df.to_excel(writer, sheet_name=\"Sheet1\", index=False, startrow=0)\n",
    "\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[\"Sheet1\"]\n",
    "\n",
    "        # defining column settings\n",
    "        (max_row, max_col) = df.shape\n",
    "        column_settings = [{\"header\": col} for col in df.columns]\n",
    "\n",
    "        # Define table range & add table\n",
    "        worksheet.add_table(0, 0, max_row, max_col -1,{\n",
    "            \"columns\": column_settings,\n",
    "            \"name\": \"Table1\",\n",
    "    })\n",
    "\n",
    "def records(df):\n",
    "    file_path = \"records.xlsx\"\n",
    "\n",
    "\n",
    "    if os.path.exists(file_path):            \n",
    "        excel_records = pd.read_excel(file_path, sheet_name=\"Sheet1\", parse_dates=[\"Date\"])\n",
    "        latest_date = excel_records[\"Date\"].max()\n",
    "\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        new_data = df[df[\"Date\"] > pd.to_datetime(latest_date)]\n",
    "        new_data = new_data.reset_index(drop=True)\n",
    "\n",
    "        # Format 'Date' column as yyyy-mm-dd string\n",
    "        new_data['Date'] = new_data['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        print(f\"New data since {latest_date}:\")\n",
    "        print(new_data.head())\n",
    "\n",
    "        if not new_data.empty:\n",
    "            workbook = load_workbook(file_path)\n",
    "            sheet = workbook.active\n",
    "            start_row = sheet.max_row\n",
    "            mode = \"a\"\n",
    "            sheet_exists_option = {\"if_sheet_exists\": \"overlay\"}\n",
    "            write_header = False\n",
    "                # Use unpacking to only include 'if_sheet_exists' when needed\n",
    "            with pd.ExcelWriter(file_path, engine=\"openpyxl\", mode=mode, **sheet_exists_option) as writer:\n",
    "                new_data.to_excel(writer, sheet_name=\"Sheet1\", index=False, startrow=start_row,header=write_header)\n",
    "            print(f\"Data appended to {file_path}\")\n",
    "        else:\n",
    "            print(\"there is no new data to append\")\n",
    "    else:\n",
    "        # Format 'Date column as yyyy-mm-dd string\n",
    "        df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        start_row = 0\n",
    "        mode = \"w\"\n",
    "        sheet_exists_option = {}\n",
    "        write_header = True\n",
    "        # Use unpacking to only include 'if_sheet_exists' when needed\n",
    "        with pd.ExcelWriter(file_path, engine=\"openpyxl\", mode=mode, **sheet_exists_option) as writer:\n",
    "            df.to_excel(writer, sheet_name=\"Sheet1\", index=False, startrow=start_row,header=write_header)\n",
    "        print(f\"Data written to {file_path}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def export_data(df):\n",
    "    new_records(df)\n",
    "    records(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
